{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "20c3e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98efbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = True\n",
    "drop_card_frac = None\n",
    "\n",
    "if train_data:\n",
    "    load_path = \"../../data/fraudTrain.csv\"\n",
    "    save_dir = \"graphs/train\"\n",
    "else:\n",
    "    load_path = \"../../data/fraudTest.csv\"\n",
    "    save_dir = \"graphs/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b923ef",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "45c37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_date_trans_time    datetime64[ns]\n",
      "cc_num                           object\n",
      "merchant                       category\n",
      "category                       category\n",
      "amt                             float64\n",
      "first                          category\n",
      "last                           category\n",
      "gender                         category\n",
      "street                         category\n",
      "city                           category\n",
      "state                          category\n",
      "zip                            category\n",
      "lat                             float64\n",
      "long                            float64\n",
      "city_pop                          int64\n",
      "job                            category\n",
      "dob                      datetime64[ns]\n",
      "trans_num                        object\n",
      "unix_time                         int64\n",
      "merch_lat                       float64\n",
      "merch_long                      float64\n",
      "is_fraud                          int64\n",
      "unix_trans_time                   int64\n",
      "age                             float64\n",
      "nb_categories                     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_264026/247072500.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv(load_path)\n",
    "\n",
    "# Drop the column named 'Unnamed: 0' (unnecessary index column)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Convert date/time columns\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "# Keep IDs as string/object\n",
    "df['cc_num'] = df['cc_num'].astype(str)\n",
    "df['trans_num'] = df['trans_num'].astype(str)\n",
    "\n",
    "# Convert categorical/text columns\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', \n",
    "                    'street', 'city', 'state', 'zip', 'job']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert to Unix timestamp (in seconds)\n",
    "df['unix_trans_time'] = df['trans_date_trans_time'].astype('int64') // 10**9\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25 # account for leap years\n",
    "\n",
    "# Compute number of distinct categories per merchant\n",
    "merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n",
    "# Add it as a new column\n",
    "df[\"nb_categories\"] = merchant_category_counts\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540bb7c",
   "metadata": {},
   "source": [
    "# Create node ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "98ed9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric IDs for graph nodes (cards, merchants, transactions)\n",
    "\n",
    "# treat each unique card number as a category\n",
    "card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "# Add a new column card_id\n",
    "df[\"card_id\"] = card_ids\n",
    "\n",
    "# treat each unique merchant as a category\n",
    "merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "# Add a new column merchant_id\n",
    "df[\"merchant_id\"] = merchant_ids\n",
    "\n",
    "# Each row is one transaction\n",
    "df[\"transaction_id\"] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2ac4030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction :  555719\n",
      "Number of cards :  924\n",
      "Number of merchants :  693\n"
     ]
    }
   ],
   "source": [
    "# Number of transaction nodes\n",
    "print(\"Number of transaction : \", len(df))\n",
    "\n",
    "# Count how many unique cards\n",
    "print(\"Number of cards : \", card_ids.nunique())\n",
    "\n",
    "# Count how many unique merchant\n",
    "print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca083d",
   "metadata": {},
   "source": [
    "## Sous-échantillonner le dataset par carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d6dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cards(df, card_col=\"card_id\", frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Garde toutes les transactions d'une fraction des cartes.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # cartes uniques\n",
    "    cards = df[card_col].unique()\n",
    "\n",
    "    # nombre de cartes à garder\n",
    "    n_keep = int(len(cards) * (1-frac))\n",
    "\n",
    "    # échantillonnage aléatoire des cartes\n",
    "    keep_cards = rng.choice(cards, size=n_keep, replace=False)\n",
    "\n",
    "    # filtrage du dataframe\n",
    "    df_kept = df[df[card_col].isin(keep_cards)].copy()\n",
    "\n",
    "    return df_kept\n",
    "\n",
    "if drop_card_frac:\n",
    "    df = drop_cards(df, frac=drop_card_frac)\n",
    "\n",
    "    # treat each unique card number as a category\n",
    "    card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "    # Add a new column card_id\n",
    "    df[\"card_id\"] = card_ids\n",
    "    # treat each unique merchant as a category\n",
    "    merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "    # Add a new column merchant_id\n",
    "    df[\"merchant_id\"] = merchant_ids\n",
    "    # Each row is one transaction\n",
    "    df[\"transaction_id\"] = range(len(df))\n",
    "\n",
    "    # Number of transaction nodes\n",
    "    print(\"Number of transaction : \", len(df))\n",
    "    # Count how many unique cards\n",
    "    print(\"Number of cards : \", card_ids.nunique())\n",
    "    # Count how many unique merchant\n",
    "    print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a7c",
   "metadata": {},
   "source": [
    "# Build node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dbf38b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fenêtre temporelle par défaut (en secondes) pour les premières transactions\n",
    "FEATURE_WINDOW = 3600\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Encodage des variables catégorielles\n",
    "# ----------------------------------------------------\n",
    "df[\"category_idx\"] = df[\"category\"].astype(\"category\").cat.codes\n",
    "df[\"gender_idx\"] = df[\"gender\"].astype(\"category\").cat.codes\n",
    "df[\"job_idx\"] = df[\"job\"].astype(\"category\").cat.codes\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Features temporelles\n",
    "# ----------------------------------------------------\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"trans_date_trans_time\"].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "# ANTI-FRAUDE : transactions nocturnes\n",
    "df[\"is_night_tx\"] = df[\"hour\"].between(0, 5).astype(int)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Tri global pour toutes les features historiques\n",
    "# ----------------------------------------------------\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Jump de temps depuis la transaction précédente\n",
    "# ----------------------------------------------------\n",
    "df[\"card_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"card_id\")[\"unix_trans_time\"].diff().fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Montant historique de la carte\n",
    "# ----------------------------------------------------\n",
    "df[\"card_amt_mean\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().mean().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_std\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().std().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_max\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().max().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_min\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().min().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "# Écarts au max/min historique\n",
    "df[\"amt_minus_prev_max\"] = df[\"amt\"] - df[\"card_amt_max\"]\n",
    "df[\"amt_minus_prev_min\"] = df[\"amt\"] - df[\"card_amt_min\"]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Z-score du montant\n",
    "# ----------------------------------------------------\n",
    "MIN_TX = 2\n",
    "df[\"card_tx_count\"] = df.groupby(\"card_id\").cumcount()\n",
    "df[\"amt_zscore\"] = np.where(\n",
    "    df[\"card_tx_count\"] < MIN_TX,\n",
    "    0,\n",
    "    (df[\"amt\"] - df[\"card_amt_mean\"]) / (df[\"card_amt_std\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# ANTI-FRAUDE : ratios plus forts que le z-score\n",
    "df[\"amt_vs_card_mean_ratio\"] = np.where(\n",
    "    df[\"card_amt_mean\"] > 0,\n",
    "    df[\"amt\"] / (df[\"card_amt_mean\"] + 1e-6),\n",
    "    1.0\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Features de fréquences\n",
    "# ----------------------------------------------------\n",
    "df[\"unix_trans_time_dt\"] = pd.to_datetime(\n",
    "    df[\"unix_trans_time\"],\n",
    "    unit=\"s\"\n",
    ")\n",
    "\n",
    "df[\"tx_count_1h\"] = (\n",
    "    df\n",
    "    .groupby(\"card_id\", group_keys=False)\n",
    "    .rolling(\"3600s\", on=\"unix_trans_time_dt\")[\"unix_trans_time_dt\"]\n",
    "    .count()\n",
    "    .shift()\n",
    "    .reset_index(drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "df[\"tx_count_24h\"] = (\n",
    "    df\n",
    "    .groupby(\"card_id\", group_keys=False)\n",
    "    .rolling(\"86400s\", on=\"unix_trans_time_dt\")[\"unix_trans_time_dt\"]\n",
    "    .count()\n",
    "    .shift()\n",
    "    .reset_index(drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Somme des montants récents\n",
    "# ----------------------------------------------------\n",
    "df[\"amt_sum_1h\"] = (\n",
    "    df\n",
    "    .groupby(\"card_id\", group_keys=False)\n",
    "    .rolling(\"3600s\", on=\"unix_trans_time_dt\")[\"amt\"]\n",
    "    .sum()\n",
    "    .shift()\n",
    "    .reset_index(drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Changement de catégorie\n",
    "# ----------------------------------------------------\n",
    "df[\"prev_category\"] = df.groupby(\"card_id\")[\"category_idx\"].shift()\n",
    "\n",
    "df[\"is_category_shift\"] = (\n",
    "    (df[\"category_idx\"] != df[\"prev_category\"]) &\n",
    "    df[\"prev_category\"].notna()\n",
    ").astype(int)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Distances géographiques\n",
    "# ----------------------------------------------------\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # rayon Terre en km\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Coordonnées du marchand précédent\n",
    "df[\"prev_merch_lat\"] = df.groupby(\"card_id\")[\"merch_lat\"].shift()\n",
    "df[\"prev_merch_long\"] = df.groupby(\"card_id\")[\"merch_long\"].shift()\n",
    "\n",
    "# Distance à la transaction précédente\n",
    "df[\"dist_from_prev_tx\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"prev_merch_lat\"], df[\"prev_merch_long\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Distance par rapport à l'adresse du propriétaire\n",
    "df[\"dist_from_home\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"lat\"], df[\"long\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ANTI-FRAUDE : vitesse implicite\n",
    "# ----------------------------------------------------\n",
    "df[\"geo_speed_kmh\"] = np.where(\n",
    "    df[\"card_time_since_prev_tx\"] > 0,\n",
    "    df[\"dist_from_prev_tx\"] / (df[\"card_time_since_prev_tx\"] / 3600),\n",
    "    0\n",
    ")\n",
    "\n",
    "df[\"is_impossible_travel\"] = (df[\"geo_speed_kmh\"] > 900).astype(int)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Merchant features\n",
    "# ----------------------------------------------------\n",
    "# Nouveau merchant pour la carte\n",
    "df[\"is_new_merchant\"] = df.groupby(\"card_id\")[\"merchant\"].transform(lambda x: ~x.duplicated()).astype(int)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Création d'un DataFrame dédié aux features merchant\n",
    "# ----------------------------------------------------\n",
    "df_merchant = (\n",
    "    df[[\"transaction_id\", \"merchant_id\", \"unix_trans_time\", \"amt\"]]\n",
    "    # Tri indispensable pour garantir la cohérence temporelle\n",
    "    .sort_values([\"merchant_id\", \"unix_trans_time\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Temps écoulé depuis la transaction précédente\n",
    "# chez le même merchant\n",
    "# ----------------------------------------------------\n",
    "df_merchant[\"merchant_time_since_prev_tx\"] = (\n",
    "    df_merchant\n",
    "    .groupby(\"merchant_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Montant moyen historique du merchant (jusqu'à t-1)\n",
    "# ----------------------------------------------------\n",
    "df_merchant[\"merchant_avg_amt\"] = (\n",
    "    df_merchant\n",
    "    .groupby(\"merchant_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ANTI-FRAUDE : montant atypique pour le merchant\n",
    "# ----------------------------------------------------\n",
    "df_merchant[\"amt_vs_merchant_avg_ratio\"] = np.where(\n",
    "    df_merchant[\"merchant_avg_amt\"] > 0,\n",
    "    df_merchant[\"amt\"] / (df_merchant[\"merchant_avg_amt\"] + 1e-6),\n",
    "    1.0\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Jointure avec le DataFrame principal\n",
    "# ----------------------------------------------------\n",
    "df = df.merge(\n",
    "    df_merchant[\n",
    "        [\"transaction_id\", \"merchant_time_since_prev_tx\", \"merchant_avg_amt\", \"amt_vs_merchant_avg_ratio\"]\n",
    "    ],\n",
    "    on=\"transaction_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a5bc4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_data:\n",
    "    df.to_csv(\"train_df.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    df.to_csv(\"test_df.csv\", index=False)\n",
    "\n",
    "if train_data:\n",
    "    # Garder uniquement les transactions non frauduleuses\n",
    "    df = df[df['is_fraud'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7434",
   "metadata": {},
   "source": [
    "# Create PyG graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ac6c49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, EDGE_WINDOW = 3600 * 24 * 7): # 7 jours\n",
    "    \"\"\"\n",
    "    Crée un graphe PyG à partir d'un dataframe df.\n",
    "    \"\"\"\n",
    "    # Node features\n",
    "    node_features = torch.tensor(\n",
    "        df[[\n",
    "            # Transaction features\n",
    "            \"amt\",\n",
    "            \"hour\",\n",
    "            \"is_night_tx\",\n",
    "            \"dayofweek\",\n",
    "            \"is_weekend\",\n",
    "            \"age\",\n",
    "            \"is_new_merchant\",\n",
    "            # \"card_time_since_prev_tx\",\n",
    "            # \"dist_from_prev_tx\",\n",
    "            \"dist_from_home\",\n",
    "            \"is_impossible_travel\",\n",
    "            \"category_idx\",\n",
    "            \"is_category_shift\",\n",
    "\n",
    "            # Card features\n",
    "            \"amt_zscore\",\n",
    "            \"amt_vs_card_mean_ratio\",\n",
    "            \"amt_minus_prev_max\",\n",
    "            \"amt_minus_prev_min\",\n",
    "            \"card_amt_mean\",\n",
    "            \"card_amt_std\",\n",
    "            \"tx_count_1h\",\n",
    "            \"tx_count_24h\",\n",
    "            \"amt_sum_1h\",\n",
    "            \n",
    "            # Identity\n",
    "            \"gender_idx\", \"job_idx\",\n",
    "            \"city_pop\",\n",
    "\n",
    "            # Mercahnt feature\n",
    "            \"merchant_avg_amt\",\n",
    "            \"merchant_time_since_prev_tx\",\n",
    "            \"amt_vs_merchant_avg_ratio\",\n",
    "            \n",
    "        ]].values,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    node_labels = torch.tensor(\n",
    "        df[\"is_fraud\"].values,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "\n",
    "    # Mapping transaction_id -> index PyG\n",
    "    tx2idx = {tx: i for i, tx in enumerate(df[\"transaction_id\"].values)}\n",
    "\n",
    "    # Create edges\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "\n",
    "    # Création des arêtes pour transactions de la même carte\n",
    "    # On regroupe les transactions par carte (card_id)\n",
    "    # puis on relie les transactions consécutives dans la fenêtre EDGE_WINDOW\n",
    "\n",
    "    for _, group in df.groupby(\"card_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        \n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t  = group[\"card_time_since_prev_tx\"].values\n",
    "        amt = group[\"amt\"].values\n",
    "        geo = group[\"dist_from_prev_tx\"].values\n",
    "        n = len(tx)\n",
    "\n",
    "        for i in range(n-1):\n",
    "            j = i+1\n",
    "            dt = t[j]\n",
    "            if dt <= EDGE_WINDOW:\n",
    "                # On ajoute une arête bidirectionnelle\n",
    "                edges.append([tx2idx[tx[i]], tx2idx[tx[j]]])\n",
    "                \n",
    "                # Attribut de l'arête\n",
    "                edge_attrs.append([\n",
    "                    # 1.0,                     # same_card\n",
    "                    # 0.0,                     # same_merchant\n",
    "                    np.log1p(dt),              # delta time\n",
    "                    abs(amt[j] - amt[i]),      # amount diff\n",
    "                    amt[j] / (amt[i] + 1e-6),  # amount ratio\n",
    "                    geo[j]                     # geo jump\n",
    "                ])\n",
    "\n",
    "\n",
    "\n",
    "    # Création des arêtes pour transactions du même merchant\n",
    "    # Même logique que pour les cartes\n",
    "    # On relie les transactions consécutives chez le même marchand\n",
    "    # for _, group in df.groupby(\"merchant_id\"):\n",
    "    #     # Tri chronologique des transactions\n",
    "    #     group = group.sort_values(\"unix_trans_time\")\n",
    "        \n",
    "    #     tx = group[\"transaction_id\"].values\n",
    "    #     t  = group[\"unix_trans_time\"].values\n",
    "    #     amt = group[\"amt\"].values\n",
    "    #     n = len(tx)\n",
    "\n",
    "    #     for i in range(n-1):\n",
    "    #         j = i+1\n",
    "    #         dt = t[j] - t[i]\n",
    "\n",
    "    #         if dt <= EDGE_WINDOW:\n",
    "    #             edges.append([tx2idx[tx[i]], tx2idx[tx[j]]])  # i -> j\n",
    "    #             # Attribut de l'arête : [same_card, same_merchant]\n",
    "    #             edge_attrs.append([\n",
    "    #                 0.0,                     # same_card\n",
    "    #                 1.0,                     # same_merchant\n",
    "    #                 np.log1p(dt),\n",
    "    #                 abs(amt[j] - amt[i]),\n",
    "    #                 amt[j] / (amt[i] + 1e-6),\n",
    "    #                 0.0                      # geo jump inconnu\n",
    "    #             ])\n",
    "\n",
    "    \n",
    "    # Assemble Data\n",
    "    # Conversion des arêtes et attributs en tenseurs PyTorch\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attrs = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    tx_ids = torch.tensor(df[\"transaction_id\"].values,dtype=torch.long)\n",
    "\n",
    "    # Création du graphe PyTorch Geometric\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attrs,\n",
    "        y=node_labels,\n",
    "        tx_id=tx_ids\n",
    "    )\n",
    "\n",
    "    data.num_nodes = node_features.size(0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4c75c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 6 ---\n",
      "Data(x=[30058, 26], edge_index=[2, 29149], edge_attr=[29149, 4], y=[30058], tx_id=[30058], num_nodes=30058)\n",
      "Nombre de noeuds: 30058\n",
      "Nombre d'arêtes: 29149\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 92232\n",
      "\n",
      "--- Batch 7 ---\n",
      "Data(x=[85848, 26], edge_index=[2, 84932], edge_attr=[84932, 4], y=[85848], tx_id=[85848], num_nodes=85848)\n",
      "Nombre de noeuds: 85848\n",
      "Nombre d'arêtes: 84932\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 234487\n",
      "\n",
      "--- Batch 8 ---\n",
      "Data(x=[88759, 26], edge_index=[2, 87849], edge_attr=[87849, 4], y=[88759], tx_id=[88759], num_nodes=88759)\n",
      "Nombre de noeuds: 88759\n",
      "Nombre d'arêtes: 87849\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 241043\n",
      "\n",
      "--- Batch 9 ---\n",
      "Data(x=[69533, 26], edge_index=[2, 68614], edge_attr=[68614, 4], y=[69533], tx_id=[69533], num_nodes=69533)\n",
      "Nombre de noeuds: 69533\n",
      "Nombre d'arêtes: 68614\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 198112\n",
      "\n",
      "--- Batch 10 ---\n",
      "Data(x=[69348, 26], edge_index=[2, 68425], edge_attr=[68425, 4], y=[69348], tx_id=[69348], num_nodes=69348)\n",
      "Nombre de noeuds: 69348\n",
      "Nombre d'arêtes: 68425\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 198103\n",
      "\n",
      "--- Batch 11 ---\n",
      "Data(x=[72635, 26], edge_index=[2, 71719], edge_attr=[71719, 4], y=[72635], tx_id=[72635], num_nodes=72635)\n",
      "Nombre de noeuds: 72635\n",
      "Nombre d'arêtes: 71719\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 204880\n",
      "\n",
      "--- Batch 12 ---\n",
      "Data(x=[139538, 26], edge_index=[2, 138628], edge_attr=[138628, 4], y=[139538], tx_id=[139538], num_nodes=139538)\n",
      "Nombre de noeuds: 139538\n",
      "Nombre d'arêtes: 138628\n",
      "Dimension features noeuds: 26\n",
      "Nombre de type d'arêtes: 345890\n",
      "\n",
      "7 graphes créés, un pour chaque période de 1 mois.\n"
     ]
    }
   ],
   "source": [
    "# durée de chaque sous-graphe (batch) en mois\n",
    "nb_months = 1\n",
    "\n",
    "# Découpage par nb_months\n",
    "graphs = []\n",
    "df[\"batch_index\"] = df[\"unix_trans_time\"].apply(lambda x: (pd.to_datetime(x, unit='s').month - 1)//nb_months + 1)\n",
    "for period, period_df in df.groupby(\"batch_index\"):\n",
    "    graph = create_graph(period_df.reset_index(drop=True))\n",
    "    graphs.append(graph)\n",
    "    print(f\"--- Batch {period} ---\")\n",
    "    print(graph)\n",
    "    print(\"Nombre de noeuds:\", graph.num_nodes)\n",
    "    print(\"Nombre d'arêtes:\", graph.num_edges)\n",
    "    print(\"Dimension features noeuds:\", graph.x.shape[1])\n",
    "    print(\"Nombre de type d'arêtes:\", len(torch.unique(graph.edge_attr)))\n",
    "    print()\n",
    "\n",
    "print(f\"{len(graphs)} graphes créés, un pour chaque période de {nb_months} mois.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6e6d025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe batch 1 enregistré : graphs/test/graph_batch_1.pt\n",
      "Graphe batch 2 enregistré : graphs/test/graph_batch_2.pt\n",
      "Graphe batch 3 enregistré : graphs/test/graph_batch_3.pt\n",
      "Graphe batch 4 enregistré : graphs/test/graph_batch_4.pt\n",
      "Graphe batch 5 enregistré : graphs/test/graph_batch_5.pt\n",
      "Graphe batch 6 enregistré : graphs/test/graph_batch_6.pt\n",
      "Graphe batch 7 enregistré : graphs/test/graph_batch_7.pt\n"
     ]
    }
   ],
   "source": [
    "# Save to a file\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, graph in enumerate(graphs, start=1):\n",
    "    file_path = os.path.join(save_dir, f\"graph_batch_{i}.pt\")\n",
    "    torch.save(graph, file_path)\n",
    "    print(f\"Graphe batch {i} enregistré : {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGAE",
   "language": "python",
   "name": "vgae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
