{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c3e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b923ef",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_date_trans_time    datetime64[ns]\n",
      "cc_num                           object\n",
      "merchant                       category\n",
      "category                       category\n",
      "amt                             float64\n",
      "first                          category\n",
      "last                           category\n",
      "gender                         category\n",
      "street                         category\n",
      "city                           category\n",
      "state                          category\n",
      "zip                            category\n",
      "lat                             float64\n",
      "long                            float64\n",
      "city_pop                          int64\n",
      "job                            category\n",
      "dob                      datetime64[ns]\n",
      "trans_num                        object\n",
      "unix_time                         int64\n",
      "merch_lat                       float64\n",
      "merch_long                      float64\n",
      "is_fraud                          int64\n",
      "unix_trans_time                   int64\n",
      "age                             float64\n",
      "nb_categories                     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21934/481264350.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the data\n",
    "# df = pd.read_csv(\"../data/fraudTrain.csv\")\n",
    "df = pd.read_csv(\"../data/fraudTest.csv\")\n",
    "\n",
    "# Drop the column named 'Unnamed: 0' (unnecessary index column)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Convert date/time columns\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "# Keep IDs as string/object\n",
    "df['cc_num'] = df['cc_num'].astype(str)\n",
    "df['trans_num'] = df['trans_num'].astype(str)\n",
    "\n",
    "# Convert categorical/text columns\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', \n",
    "                    'street', 'city', 'state', 'zip', 'job']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert to Unix timestamp (in seconds)\n",
    "df['unix_trans_time'] = df['trans_date_trans_time'].astype('int64') // 10**9\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25 # account for leap years\n",
    "\n",
    "# Compute number of distinct categories per merchant\n",
    "merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n",
    "# Add it as a new column\n",
    "df[\"nb_categories\"] = merchant_category_counts\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540bb7c",
   "metadata": {},
   "source": [
    "# Create node ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ed9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric IDs for graph nodes (cards, merchants, transactions)\n",
    "\n",
    "# treat each unique card number as a category\n",
    "card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "# Add a new column card_id\n",
    "df[\"card_id\"] = card_ids\n",
    "\n",
    "# treat each unique merchant as a category\n",
    "merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "# Add a new column merchant_id\n",
    "df[\"merchant_id\"] = merchant_ids\n",
    "\n",
    "# Each row is one transaction\n",
    "df[\"transaction_id\"] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac4030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction :  555719\n",
      "Number of cards :  924\n",
      "Number of merchants :  693\n"
     ]
    }
   ],
   "source": [
    "# Number of transaction nodes\n",
    "print(\"Number of transaction : \", len(df))\n",
    "\n",
    "# Count how many unique cards\n",
    "print(\"Number of cards : \", card_ids.nunique())\n",
    "\n",
    "# Count how many unique merchant\n",
    "print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a7c",
   "metadata": {},
   "source": [
    "# Build node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf38b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fenêtre temporelle (en secondes) utilisée comme valeur par défaut\n",
    "# lorsque aucune transaction précédente n’existe\n",
    "FEATURE_WINDOW = 3600  # 1 heure\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "df[\"category_idx\"] = df[\"category\"].astype(\"category\").cat.codes\n",
    "df[\"gender_idx\"] = df[\"gender\"].astype(\"category\").cat.codes\n",
    "df[\"job_idx\"] = df[\"job\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Features temporelles\n",
    "# Ces variables capturent les comportements cycliques et\n",
    "# les habitudes de consommation\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"trans_date_trans_time\"].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "# Dynamique temporelle de la carte\n",
    "# Tri chronologique des transactions par carte\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"])\n",
    "# Temps écoulé (en secondes) depuis la transaction précédente\n",
    "# pour la même carte\n",
    "# Un intervalle très court peut indiquer une activité anormale\n",
    "df[\"card_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"card_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "\n",
    "# Déviation du montant par rapport à l’historique de la carte\n",
    "# Montant moyen historique de la carte (jusqu’à t-1)\n",
    "df[\"card_amt_mean\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "# Écart-type historique des montants de la carte (jusqu’à t-1)\n",
    "df[\"card_amt_std\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .std()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "# Remplacement des valeurs manquantes (premières transactions)\n",
    "df[[\"card_amt_mean\", \"card_amt_std\"]] = df[[\"card_amt_mean\", \"card_amt_std\"]].fillna(0)\n",
    "\n",
    "# Z-score : mesure à quel point le montant est atypique pour cette carte\n",
    "df[\"amt_zscore\"] = (\n",
    "    (df[\"amt\"] - df[\"card_amt_mean\"]) /\n",
    "    (df[\"card_amt_std\"] + 1e-6)\n",
    ")\n",
    "\n",
    "\n",
    "# Distance géographique entre transactions consécutives\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # rayon Terre en km\n",
    "\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat / 2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "\n",
    "# Tri chronologique des transactions par carte\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"])\n",
    "# Coordonnées du marchand de la transaction précédente\n",
    "df[\"prev_merch_lat\"] = df.groupby(\"card_id\")[\"merch_lat\"].shift()\n",
    "df[\"prev_merch_long\"] = df.groupby(\"card_id\")[\"merch_long\"].shift()\n",
    "# Distance géographique entre deux transactions consécutives\n",
    "# Une grande distance sur un temps court est un fort signal de fraude\n",
    "df[\"geo_dist\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"prev_merch_lat\"], df[\"prev_merch_long\"]\n",
    ")\n",
    "# Valeur nulle pour la première transaction\n",
    "df[\"geo_dist\"] = df[\"geo_dist\"].fillna(0)\n",
    "\n",
    "# Nouveau merchant pour la carte\n",
    "# Indique si ce marchand n’a jamais été utilisé auparavant\n",
    "# par cette carte (1 = nouveau marchand)\n",
    "df[\"is_new_merchant\"] = (\n",
    "    df.groupby(\"card_id\")[\"merchant\"]\n",
    "    .transform(lambda x: ~x.duplicated())\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Dynamique temporelle du merchant\n",
    "# Temps écoulé depuis la dernière transaction chez ce marchand\n",
    "# Un afflux soudain de transactions peut être suspect\n",
    "df[\"merchant_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"merchant_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# Montant moyen historique des transactions du merchant\n",
    "# (calculé uniquement sur le passé)\n",
    "df[\"merchant_avg_amt\"] = (\n",
    "    df.groupby(\"merchant_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7434",
   "metadata": {},
   "source": [
    "# Create PyG graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6c49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, EDGE_WINDOW = 3600 * 2):\n",
    "    \"\"\"\n",
    "    Crée un graphe PyG à partir d'un dataframe df.\n",
    "    \"\"\"\n",
    "    # Node features\n",
    "    node_features = torch.tensor(\n",
    "        df[[\n",
    "            # Transaction features\n",
    "            \"amt\",\n",
    "            \"card_time_since_prev_tx\",\n",
    "            \"hour\",\n",
    "            \"dayofweek\",\n",
    "            \"is_weekend\",\n",
    "            # \"age\",\n",
    "            # \"is_new_merchant\",\n",
    "            # \"geo_dist\",\n",
    "            # \"merch_lat\", \"merch_long\",\n",
    "            # \"category_idx\",\n",
    "\n",
    "            # Card features\n",
    "            \"amt_zscore\",\n",
    "            # \"card_amt_mean\",\n",
    "            # \"card_amt_std\",\n",
    "            # \"gender_idx\", \"job_idx\",\n",
    "            # \"lat\", \"long\", \"city_pop\",\n",
    "\n",
    "            # Mercahnt feature\n",
    "            # \"merchant_avg_amt\",\n",
    "            # \"merchant_time_since_prev_tx\"\n",
    "            \n",
    "        ]].values,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    # Mapping transaction_id -> index PyG\n",
    "    tx2idx = {tx: i for i, tx in enumerate(df[\"transaction_id\"].values)}\n",
    "\n",
    "    # Create edges\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "\n",
    "    # Création des arêtes pour transactions de la même carte\n",
    "    # On regroupe les transactions par carte (card_id)\n",
    "    # puis on relie les transactions consécutives dans la fenêtre EDGE_WINDOW\n",
    "    for _, group in df.groupby(\"card_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t = group[\"unix_trans_time\"].values\n",
    "\n",
    "        # Parcours des transactions consécutives\n",
    "        for i in range(len(tx) - 1):\n",
    "            # Si deux transactions sont assez proches dans le temps\n",
    "            if t[i+1] - t[i] <= EDGE_WINDOW:\n",
    "                # On ajoute une arête bidirectionnelle\n",
    "                edges.append([tx2idx[tx[i]], tx2idx[tx[i+1]]])\n",
    "                edges.append([tx2idx[tx[i+1]], tx2idx[tx[i]]])\n",
    "\n",
    "                # Attribut de l'arête : [same_card, same_merchant]\n",
    "                # Ici, same_card = 1, same_merchant = \n",
    "                edge_attrs.append([1, 0])\n",
    "                edge_attrs.append([1, 0])\n",
    "\n",
    "\n",
    "    # Création des arêtes pour transactions du même merchant\n",
    "    # Même logique que pour les cartes\n",
    "    # On relie les transactions consécutives chez le même marchand\n",
    "    for _, group in df.groupby(\"merchant_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t = group[\"unix_trans_time\"].values\n",
    "\n",
    "        for i in range(len(tx) - 1):\n",
    "            # Si deux transactions sont assez proches dans le temps\n",
    "            if t[i+1] - t[i] <= EDGE_WINDOW:\n",
    "                # Arêtes bidirectionnelles\n",
    "                edges.append([tx2idx[tx[i]], tx2idx[tx[i+1]]])\n",
    "                edges.append([tx2idx[tx[i+1]], tx2idx[tx[i]]])\n",
    "\n",
    "                # Attribut de l'arête : [same_card, same_merchant]\n",
    "                # Ici, same_card = 0, same_merchant = 1\n",
    "                edge_attrs.append([0, 1])\n",
    "                edge_attrs.append([0, 1])\n",
    "\n",
    "    \n",
    "    # Assemble Data\n",
    "    # Conversion des arêtes et attributs en tenseurs PyTorch\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    # Création du graphe PyTorch Geometric\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "    )\n",
    "\n",
    "    data.num_nodes = node_features.size(0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c75c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 6 ---\n",
      "Data(x=[30058, 6], edge_index=[2, 48186], edge_attr=[48186, 2], num_nodes=30058)\n",
      "Nombre de noeuds: 30058\n",
      "Nombre d'arêtes: 48186\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 7 ---\n",
      "Data(x=[85848, 6], edge_index=[2, 126638], edge_attr=[126638, 2], num_nodes=85848)\n",
      "Nombre de noeuds: 85848\n",
      "Nombre d'arêtes: 126638\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 8 ---\n",
      "Data(x=[88759, 6], edge_index=[2, 134288], edge_attr=[134288, 2], num_nodes=88759)\n",
      "Nombre de noeuds: 88759\n",
      "Nombre d'arêtes: 134288\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 9 ---\n",
      "Data(x=[69533, 6], edge_index=[2, 89804], edge_attr=[89804, 2], num_nodes=69533)\n",
      "Nombre de noeuds: 69533\n",
      "Nombre d'arêtes: 89804\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 10 ---\n",
      "Data(x=[69348, 6], edge_index=[2, 87168], edge_attr=[87168, 2], num_nodes=69348)\n",
      "Nombre de noeuds: 69348\n",
      "Nombre d'arêtes: 87168\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 11 ---\n",
      "Data(x=[72635, 6], edge_index=[2, 100270], edge_attr=[100270, 2], num_nodes=72635)\n",
      "Nombre de noeuds: 72635\n",
      "Nombre d'arêtes: 100270\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "--- Batch 12 ---\n",
      "Data(x=[139538, 6], edge_index=[2, 283050], edge_attr=[283050, 2], num_nodes=139538)\n",
      "Nombre de noeuds: 139538\n",
      "Nombre d'arêtes: 283050\n",
      "Dimension features noeuds: 6\n",
      "Dimension features arêtes: 2\n",
      "\n",
      "7 graphes créés, un pour chaque période de 1 mois.\n"
     ]
    }
   ],
   "source": [
    "# durée de chaque sous-graphe (batch) en mois\n",
    "nb_months = 1\n",
    "\n",
    "# Découpage par nb_months\n",
    "graphs = []\n",
    "df[\"batch_index\"] = df[\"unix_trans_time\"].apply(lambda x: (pd.to_datetime(x, unit='s').month - 1)//nb_months + 1)\n",
    "for period, period_df in df.groupby(\"batch_index\"):\n",
    "    graph = create_graph(period_df.reset_index(drop=True))\n",
    "    graphs.append(graph)\n",
    "    print(f\"--- Batch {period} ---\")\n",
    "    print(graph)\n",
    "    print(\"Nombre de noeuds:\", graph.num_nodes)\n",
    "    print(\"Nombre d'arêtes:\", graph.num_edges)\n",
    "    print(\"Dimension features noeuds:\", graph.x.shape[1])\n",
    "    print(\"Dimension features arêtes:\", graph.edge_attr.shape[1])\n",
    "    print()\n",
    "\n",
    "print(f\"{len(graphs)} graphes créés, un pour chaque période de {nb_months} mois.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6d025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe batch 1 enregistré : graphs/test/graph_batch_1.pt\n",
      "Graphe batch 2 enregistré : graphs/test/graph_batch_2.pt\n",
      "Graphe batch 3 enregistré : graphs/test/graph_batch_3.pt\n",
      "Graphe batch 4 enregistré : graphs/test/graph_batch_4.pt\n",
      "Graphe batch 5 enregistré : graphs/test/graph_batch_5.pt\n",
      "Graphe batch 6 enregistré : graphs/test/graph_batch_6.pt\n",
      "Graphe batch 7 enregistré : graphs/test/graph_batch_7.pt\n"
     ]
    }
   ],
   "source": [
    "# Save to a file\n",
    "# Make sure the folder exists\n",
    "# save_dir = \"graphs/train\"\n",
    "save_dir = \"graphs/test\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, graph in enumerate(graphs, start=1):\n",
    "    file_path = os.path.join(save_dir, f\"graph_batch_{i}.pt\")\n",
    "    torch.save(graph, file_path)\n",
    "    print(f\"Graphe batch {i} enregistré : {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGAE",
   "language": "python",
   "name": "vgae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
