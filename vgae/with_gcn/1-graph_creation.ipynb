{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c3e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98efbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = True\n",
    "drop_card_frac = None\n",
    "\n",
    "if train_data:\n",
    "    load_path = \"../../data/fraudTrain.csv\"\n",
    "    save_dir = \"graphs/train\"\n",
    "else:\n",
    "    load_path = \"../../data/fraudTest.csv\"\n",
    "    save_dir = \"graphs/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b923ef",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_date_trans_time    datetime64[ns]\n",
      "cc_num                           object\n",
      "merchant                       category\n",
      "category                       category\n",
      "amt                             float64\n",
      "first                          category\n",
      "last                           category\n",
      "gender                         category\n",
      "street                         category\n",
      "city                           category\n",
      "state                          category\n",
      "zip                            category\n",
      "lat                             float64\n",
      "long                            float64\n",
      "city_pop                          int64\n",
      "job                            category\n",
      "dob                      datetime64[ns]\n",
      "trans_num                        object\n",
      "unix_time                         int64\n",
      "merch_lat                       float64\n",
      "merch_long                      float64\n",
      "is_fraud                          int64\n",
      "unix_trans_time                   int64\n",
      "age                             float64\n",
      "nb_categories                     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72321/4178889426.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv(load_path)\n",
    "\n",
    "# Drop the column named 'Unnamed: 0' (unnecessary index column)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "if train_data:\n",
    "    # Garder uniquement les transactions non frauduleuses\n",
    "    df = df[df['is_fraud'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Convert date/time columns\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "# Keep IDs as string/object\n",
    "df['cc_num'] = df['cc_num'].astype(str)\n",
    "df['trans_num'] = df['trans_num'].astype(str)\n",
    "\n",
    "# Convert categorical/text columns\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', \n",
    "                    'street', 'city', 'state', 'zip', 'job']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert to Unix timestamp (in seconds)\n",
    "df['unix_trans_time'] = df['trans_date_trans_time'].astype('int64') // 10**9\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25 # account for leap years\n",
    "\n",
    "# Compute number of distinct categories per merchant\n",
    "merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n",
    "# Add it as a new column\n",
    "df[\"nb_categories\"] = merchant_category_counts\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540bb7c",
   "metadata": {},
   "source": [
    "# Create node ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ed9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric IDs for graph nodes (cards, merchants, transactions)\n",
    "\n",
    "# treat each unique card number as a category\n",
    "card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "# Add a new column card_id\n",
    "df[\"card_id\"] = card_ids\n",
    "\n",
    "# treat each unique merchant as a category\n",
    "merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "# Add a new column merchant_id\n",
    "df[\"merchant_id\"] = merchant_ids\n",
    "\n",
    "# Each row is one transaction\n",
    "df[\"transaction_id\"] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac4030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction :  1289169\n",
      "Number of cards :  908\n",
      "Number of merchants :  693\n"
     ]
    }
   ],
   "source": [
    "# Number of transaction nodes\n",
    "print(\"Number of transaction : \", len(df))\n",
    "\n",
    "# Count how many unique cards\n",
    "print(\"Number of cards : \", card_ids.nunique())\n",
    "\n",
    "# Count how many unique merchant\n",
    "print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca083d",
   "metadata": {},
   "source": [
    "## Sous-échantillonner le dataset par carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d6dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cards(df, card_col=\"card_id\", frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Garde toutes les transactions d'une fraction des cartes.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # cartes uniques\n",
    "    cards = df[card_col].unique()\n",
    "\n",
    "    # nombre de cartes à garder\n",
    "    n_keep = int(len(cards) * (1-frac))\n",
    "\n",
    "    # échantillonnage aléatoire des cartes\n",
    "    keep_cards = rng.choice(cards, size=n_keep, replace=False)\n",
    "\n",
    "    # filtrage du dataframe\n",
    "    df_kept = df[df[card_col].isin(keep_cards)].copy()\n",
    "\n",
    "    return df_kept\n",
    "\n",
    "if drop_card_frac:\n",
    "    df = drop_cards(df, frac=drop_card_frac)\n",
    "\n",
    "    # treat each unique card number as a category\n",
    "    card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "    # Add a new column card_id\n",
    "    df[\"card_id\"] = card_ids\n",
    "    # treat each unique merchant as a category\n",
    "    merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "    # Add a new column merchant_id\n",
    "    df[\"merchant_id\"] = merchant_ids\n",
    "    # Each row is one transaction\n",
    "    df[\"transaction_id\"] = range(len(df))\n",
    "\n",
    "    # Number of transaction nodes\n",
    "    print(\"Number of transaction : \", len(df))\n",
    "    # Count how many unique cards\n",
    "    print(\"Number of cards : \", card_ids.nunique())\n",
    "    # Count how many unique merchant\n",
    "    print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a7c",
   "metadata": {},
   "source": [
    "# Build node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf38b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fenêtre temporelle par défaut (en secondes) pour les premières transactions\n",
    "FEATURE_WINDOW = 3600\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Encodage des variables catégorielles\n",
    "# ----------------------------------------------------\n",
    "df[\"category_idx\"] = df[\"category\"].astype(\"category\").cat.codes\n",
    "df[\"gender_idx\"] = df[\"gender\"].astype(\"category\").cat.codes\n",
    "df[\"job_idx\"] = df[\"job\"].astype(\"category\").cat.codes\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Features temporelles\n",
    "# ----------------------------------------------------\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"trans_date_trans_time\"].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Tri global pour toutes les features historiques\n",
    "# ----------------------------------------------------\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Jump de temps depuis la transaction précédente\n",
    "# ----------------------------------------------------\n",
    "df[\"card_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"card_id\")[\"unix_trans_time\"].diff().fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Montant historique de la carte\n",
    "# ----------------------------------------------------\n",
    "df[\"card_amt_mean\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().mean().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_std\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().std().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_max\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().max().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "df[\"card_amt_min\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"].expanding().min().shift().reset_index(level=0, drop=True)\n",
    ").fillna(0)\n",
    "\n",
    "# Écarts au max/min historique\n",
    "df[\"amt_minus_prev_max\"] = df[\"amt\"] - df[\"card_amt_max\"]\n",
    "df[\"amt_minus_prev_min\"] = df[\"amt\"] - df[\"card_amt_min\"]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Z-score du montant\n",
    "# ----------------------------------------------------\n",
    "MIN_TX = 2\n",
    "df[\"card_tx_count\"] = df.groupby(\"card_id\").cumcount()\n",
    "df[\"amt_zscore\"] = np.where(\n",
    "    df[\"card_tx_count\"] < MIN_TX,\n",
    "    0,\n",
    "    (df[\"amt\"] - df[\"card_amt_mean\"]) / (df[\"card_amt_std\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Distances géographiques\n",
    "# ----------------------------------------------------\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # rayon Terre en km\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Coordonnées du marchand précédent\n",
    "df[\"prev_merch_lat\"] = df.groupby(\"card_id\")[\"merch_lat\"].shift()\n",
    "df[\"prev_merch_long\"] = df.groupby(\"card_id\")[\"merch_long\"].shift()\n",
    "\n",
    "# Distance à la transaction précédente\n",
    "df[\"dist_from_prev_tx\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"prev_merch_lat\"], df[\"prev_merch_long\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Distance par rapport à l'adresse du propriétaire\n",
    "df[\"dist_from_home\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"lat\"], df[\"long\"]\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Merchant features\n",
    "# ----------------------------------------------------\n",
    "# Nouveau merchant pour la carte\n",
    "df[\"is_new_merchant\"] = df.groupby(\"card_id\")[\"merchant\"].transform(lambda x: ~x.duplicated()).astype(int)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Création d'un DataFrame dédié aux features merchant\n",
    "# ----------------------------------------------------\n",
    "df_merchant = (\n",
    "    df[[\"transaction_id\", \"merchant_id\", \"unix_trans_time\", \"amt\"]]\n",
    "    # Tri indispensable pour garantir la cohérence temporelle\n",
    "    .sort_values([\"merchant_id\", \"unix_trans_time\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Temps écoulé depuis la transaction précédente\n",
    "# chez le même merchant\n",
    "# ----------------------------------------------------\n",
    "df_merchant[\"merchant_time_since_prev_tx\"] = (\n",
    "    df_merchant\n",
    "    .groupby(\"merchant_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Montant moyen historique du merchant (jusqu'à t-1)\n",
    "# ----------------------------------------------------\n",
    "df_merchant[\"merchant_avg_amt\"] = (\n",
    "    df_merchant\n",
    "    .groupby(\"merchant_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Jointure avec le DataFrame principal\n",
    "# ----------------------------------------------------\n",
    "df = df.merge(\n",
    "    df_merchant[\n",
    "        [\"transaction_id\", \"merchant_time_since_prev_tx\", \"merchant_avg_amt\"]\n",
    "    ],\n",
    "    on=\"transaction_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7434",
   "metadata": {},
   "source": [
    "# Create PyG graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac6c49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, MAX_EDGES = 5, EDGE_WINDOW = 3600 * 24 * 7): #7 jours\n",
    "    \"\"\"\n",
    "    Crée un graphe PyG à partir d'un dataframe df.\n",
    "    \"\"\"\n",
    "    # Node features\n",
    "    node_features = torch.tensor(\n",
    "        df[[\n",
    "            # Transaction features\n",
    "            \"amt\",\n",
    "            \"hour\",\n",
    "            \"dayofweek\",\n",
    "            \"is_weekend\",\n",
    "            \"age\",\n",
    "            \"is_new_merchant\",\n",
    "            \"card_time_since_prev_tx\",\n",
    "            \"dist_from_home\",\n",
    "            \"dist_from_prev_tx\",\n",
    "            \"category_idx\",\n",
    "\n",
    "            # Card features\n",
    "            \"amt_zscore\",\n",
    "            \"amt_minus_prev_max\",\n",
    "            \"amt_minus_prev_min\",\n",
    "            \"card_amt_mean\",\n",
    "            \"card_amt_std\",\n",
    "            \"gender_idx\", \"job_idx\",\n",
    "            \"city_pop\",\n",
    "\n",
    "            # Mercahnt feature\n",
    "            \"merchant_avg_amt\",\n",
    "            \"merchant_time_since_prev_tx\"\n",
    "            \n",
    "        ]].values,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    node_labels = torch.tensor(\n",
    "        df[\"is_fraud\"].values,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "\n",
    "    # Mapping transaction_id -> index PyG\n",
    "    tx2idx = {tx: i for i, tx in enumerate(df[\"transaction_id\"].values)}\n",
    "\n",
    "    # Create edges\n",
    "    edges = []\n",
    "\n",
    "    # Création des arêtes pour transactions de la même carte\n",
    "    # On regroupe les transactions par carte (card_id)\n",
    "    # puis on relie les transactions consécutives dans la fenêtre EDGE_WINDOW\n",
    "    for _, group in df.groupby(\"card_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t = group[\"unix_trans_time\"].values\n",
    "        \n",
    "        n = len(tx)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, min(i + 1 + MAX_EDGES, n)):\n",
    "                dt = t[j] - t[i]\n",
    "                if dt <= EDGE_WINDOW:\n",
    "                    # On ajoute une arête bidirectionnelle\n",
    "                    edges.append([tx2idx[tx[i]], tx2idx[tx[j]]])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Assemble Data\n",
    "    # Conversion des arêtes et attributs en tenseurs PyTorch\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    tx_ids = torch.tensor(df[\"transaction_id\"].values,dtype=torch.long)\n",
    "\n",
    "    # Création du graphe PyTorch Geometric\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        y=node_labels,\n",
    "        tx_id=tx_ids\n",
    "    )\n",
    "\n",
    "    data.num_nodes = node_features.size(0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c75c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 1 ---\n",
      "Data(x=[103878, 20], edge_index=[2, 484877], y=[103878], tx_id=[103878], num_nodes=103878)\n",
      "Nombre de noeuds: 103878\n",
      "Nombre d'arêtes: 484877\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 2 ---\n",
      "Data(x=[96804, 20], edge_index=[2, 450227], y=[96804], tx_id=[96804], num_nodes=96804)\n",
      "Nombre de noeuds: 96804\n",
      "Nombre d'arêtes: 450227\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 3 ---\n",
      "Data(x=[142851, 20], edge_index=[2, 682676], y=[142851], tx_id=[142851], num_nodes=142851)\n",
      "Nombre de noeuds: 142851\n",
      "Nombre d'arêtes: 682676\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 4 ---\n",
      "Data(x=[134292, 20], edge_index=[2, 640228], y=[134292], tx_id=[134292], num_nodes=134292)\n",
      "Nombre de noeuds: 134292\n",
      "Nombre d'arêtes: 640228\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 5 ---\n",
      "Data(x=[145940, 20], edge_index=[2, 698731], y=[145940], tx_id=[145940], num_nodes=145940)\n",
      "Nombre de noeuds: 145940\n",
      "Nombre d'arêtes: 698731\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 6 ---\n",
      "Data(x=[143123, 20], edge_index=[2, 686912], y=[143123], tx_id=[143123], num_nodes=143123)\n",
      "Nombre de noeuds: 143123\n",
      "Nombre d'arêtes: 686912\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 7 ---\n",
      "Data(x=[86265, 20], edge_index=[2, 416502], y=[86265], tx_id=[86265], num_nodes=86265)\n",
      "Nombre de noeuds: 86265\n",
      "Nombre d'arêtes: 416502\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 8 ---\n",
      "Data(x=[86977, 20], edge_index=[2, 420359], y=[86977], tx_id=[86977], num_nodes=86977)\n",
      "Nombre de noeuds: 86977\n",
      "Nombre d'arêtes: 420359\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 9 ---\n",
      "Data(x=[70234, 20], edge_index=[2, 335365], y=[70234], tx_id=[70234], num_nodes=70234)\n",
      "Nombre de noeuds: 70234\n",
      "Nombre d'arêtes: 335365\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 10 ---\n",
      "Data(x=[68304, 20], edge_index=[2, 325825], y=[68304], tx_id=[68304], num_nodes=68304)\n",
      "Nombre de noeuds: 68304\n",
      "Nombre d'arêtes: 325825\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 11 ---\n",
      "Data(x=[70033, 20], edge_index=[2, 334579], y=[70033], tx_id=[70033], num_nodes=70033)\n",
      "Nombre de noeuds: 70033\n",
      "Nombre d'arêtes: 334579\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "--- Batch 12 ---\n",
      "Data(x=[140468, 20], edge_index=[2, 688680], y=[140468], tx_id=[140468], num_nodes=140468)\n",
      "Nombre de noeuds: 140468\n",
      "Nombre d'arêtes: 688680\n",
      "Dimension features noeuds: 20\n",
      "\n",
      "12 graphes créés, un pour chaque période de 1 mois.\n"
     ]
    }
   ],
   "source": [
    "# durée de chaque sous-graphe (batch) en mois\n",
    "nb_months = 1\n",
    "\n",
    "# Découpage par nb_months\n",
    "graphs = []\n",
    "df[\"batch_index\"] = df[\"unix_trans_time\"].apply(lambda x: (pd.to_datetime(x, unit='s').month - 1)//nb_months + 1)\n",
    "for period, period_df in df.groupby(\"batch_index\"):\n",
    "    graph = create_graph(period_df.reset_index(drop=True))\n",
    "    graphs.append(graph)\n",
    "    print(f\"--- Batch {period} ---\")\n",
    "    print(graph)\n",
    "    print(\"Nombre de noeuds:\", graph.num_nodes)\n",
    "    print(\"Nombre d'arêtes:\", graph.num_edges)\n",
    "    print(\"Dimension features noeuds:\", graph.x.shape[1])\n",
    "    print()\n",
    "\n",
    "print(f\"{len(graphs)} graphes créés, un pour chaque période de {nb_months} mois.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e6d025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe batch 1 enregistré : graphs/train/graph_batch_1.pt\n",
      "Graphe batch 2 enregistré : graphs/train/graph_batch_2.pt\n",
      "Graphe batch 3 enregistré : graphs/train/graph_batch_3.pt\n",
      "Graphe batch 4 enregistré : graphs/train/graph_batch_4.pt\n",
      "Graphe batch 5 enregistré : graphs/train/graph_batch_5.pt\n",
      "Graphe batch 6 enregistré : graphs/train/graph_batch_6.pt\n",
      "Graphe batch 7 enregistré : graphs/train/graph_batch_7.pt\n",
      "Graphe batch 8 enregistré : graphs/train/graph_batch_8.pt\n",
      "Graphe batch 9 enregistré : graphs/train/graph_batch_9.pt\n",
      "Graphe batch 10 enregistré : graphs/train/graph_batch_10.pt\n",
      "Graphe batch 11 enregistré : graphs/train/graph_batch_11.pt\n",
      "Graphe batch 12 enregistré : graphs/train/graph_batch_12.pt\n"
     ]
    }
   ],
   "source": [
    "# Save to a file\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, graph in enumerate(graphs, start=1):\n",
    "    file_path = os.path.join(save_dir, f\"graph_batch_{i}.pt\")\n",
    "    torch.save(graph, file_path)\n",
    "    print(f\"Graphe batch {i} enregistré : {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGAE",
   "language": "python",
   "name": "vgae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
