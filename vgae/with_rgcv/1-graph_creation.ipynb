{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c3e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98efbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = True\n",
    "drop_card_frac = None\n",
    "\n",
    "if train_data:\n",
    "    load_path = \"../../data/fraudTrain.csv\"\n",
    "    save_dir = \"graphs/train\"\n",
    "else:\n",
    "    load_path = \"../../data/fraudTest.csv\"\n",
    "    save_dir = \"graphs/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b923ef",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_date_trans_time    datetime64[ns]\n",
      "cc_num                           object\n",
      "merchant                       category\n",
      "category                       category\n",
      "amt                             float64\n",
      "first                          category\n",
      "last                           category\n",
      "gender                         category\n",
      "street                         category\n",
      "city                           category\n",
      "state                          category\n",
      "zip                            category\n",
      "lat                             float64\n",
      "long                            float64\n",
      "city_pop                          int64\n",
      "job                            category\n",
      "dob                      datetime64[ns]\n",
      "trans_num                        object\n",
      "unix_time                         int64\n",
      "merch_lat                       float64\n",
      "merch_long                      float64\n",
      "is_fraud                          int64\n",
      "unix_trans_time                   int64\n",
      "age                             float64\n",
      "nb_categories                     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62500/4178889426.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv(load_path)\n",
    "\n",
    "# Drop the column named 'Unnamed: 0' (unnecessary index column)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "if train_data:\n",
    "    # Garder uniquement les transactions non frauduleuses\n",
    "    df = df[df['is_fraud'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Convert date/time columns\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "# Keep IDs as string/object\n",
    "df['cc_num'] = df['cc_num'].astype(str)\n",
    "df['trans_num'] = df['trans_num'].astype(str)\n",
    "\n",
    "# Convert categorical/text columns\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', \n",
    "                    'street', 'city', 'state', 'zip', 'job']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert to Unix timestamp (in seconds)\n",
    "df['unix_trans_time'] = df['trans_date_trans_time'].astype('int64') // 10**9\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25 # account for leap years\n",
    "\n",
    "# Compute number of distinct categories per merchant\n",
    "merchant_category_counts = df.groupby(\"merchant\")[\"category\"].transform(\"nunique\")\n",
    "# Add it as a new column\n",
    "df[\"nb_categories\"] = merchant_category_counts\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540bb7c",
   "metadata": {},
   "source": [
    "# Create node ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ed9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric IDs for graph nodes (cards, merchants, transactions)\n",
    "\n",
    "# treat each unique card number as a category\n",
    "card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "# Add a new column card_id\n",
    "df[\"card_id\"] = card_ids\n",
    "\n",
    "# treat each unique merchant as a category\n",
    "merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "# Add a new column merchant_id\n",
    "df[\"merchant_id\"] = merchant_ids\n",
    "\n",
    "# Each row is one transaction\n",
    "df[\"transaction_id\"] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac4030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction :  1289169\n",
      "Number of cards :  908\n",
      "Number of merchants :  693\n"
     ]
    }
   ],
   "source": [
    "# Number of transaction nodes\n",
    "print(\"Number of transaction : \", len(df))\n",
    "\n",
    "# Count how many unique cards\n",
    "print(\"Number of cards : \", card_ids.nunique())\n",
    "\n",
    "# Count how many unique merchant\n",
    "print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca083d",
   "metadata": {},
   "source": [
    "## Sous-échantillonner le dataset par carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d6dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cards(df, card_col=\"card_id\", frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Garde toutes les transactions d'une fraction des cartes.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # cartes uniques\n",
    "    cards = df[card_col].unique()\n",
    "\n",
    "    # nombre de cartes à garder\n",
    "    n_keep = int(len(cards) * (1-frac))\n",
    "\n",
    "    # échantillonnage aléatoire des cartes\n",
    "    keep_cards = rng.choice(cards, size=n_keep, replace=False)\n",
    "\n",
    "    # filtrage du dataframe\n",
    "    df_kept = df[df[card_col].isin(keep_cards)].copy()\n",
    "\n",
    "    return df_kept\n",
    "\n",
    "if drop_card_frac:\n",
    "    df = drop_cards(df, frac=drop_card_frac)\n",
    "\n",
    "    # treat each unique card number as a category\n",
    "    card_ids = df[\"cc_num\"].astype(\"category\").cat.codes\n",
    "    # Add a new column card_id\n",
    "    df[\"card_id\"] = card_ids\n",
    "    # treat each unique merchant as a category\n",
    "    merchant_ids = df[\"merchant\"].astype(\"category\").cat.codes\n",
    "    # Add a new column merchant_id\n",
    "    df[\"merchant_id\"] = merchant_ids\n",
    "    # Each row is one transaction\n",
    "    df[\"transaction_id\"] = range(len(df))\n",
    "\n",
    "    # Number of transaction nodes\n",
    "    print(\"Number of transaction : \", len(df))\n",
    "    # Count how many unique cards\n",
    "    print(\"Number of cards : \", card_ids.nunique())\n",
    "    # Count how many unique merchant\n",
    "    print(\"Number of merchants : \", merchant_ids.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52828a7c",
   "metadata": {},
   "source": [
    "# Build node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf38b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fenêtre temporelle (en secondes) utilisée comme valeur par défaut\n",
    "# lorsque aucune transaction précédente n’existe\n",
    "FEATURE_WINDOW = 3600\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "df[\"category_idx\"] = df[\"category\"].astype(\"category\").cat.codes\n",
    "df[\"gender_idx\"] = df[\"gender\"].astype(\"category\").cat.codes\n",
    "df[\"job_idx\"] = df[\"job\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Features temporelles\n",
    "# Ces variables capturent les comportements cycliques et\n",
    "# les habitudes de consommation\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"dayofweek\"] = df[\"trans_date_trans_time\"].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "# Dynamique temporelle de la carte\n",
    "# Tri chronologique des transactions par carte\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"])\n",
    "# Temps écoulé (en secondes) depuis la transaction précédente\n",
    "# pour la même carte\n",
    "# Un intervalle très court peut indiquer une activité anormale\n",
    "df[\"card_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"card_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "\n",
    "# Déviation du montant par rapport à l’historique de la carte\n",
    "# Montant moyen historique de la carte (jusqu’à t-1)\n",
    "df[\"card_amt_mean\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "# Écart-type historique des montants de la carte (jusqu’à t-1)\n",
    "df[\"card_amt_std\"] = (\n",
    "    df.groupby(\"card_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .std()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "# Remplacement des valeurs manquantes (premières transactions)\n",
    "df[[\"card_amt_mean\", \"card_amt_std\"]] = df[[\"card_amt_mean\", \"card_amt_std\"]].fillna(0)\n",
    "\n",
    "# Z-score : mesure à quel point le montant est atypique pour cette carte\n",
    "df[\"amt_zscore\"] = (\n",
    "    (df[\"amt\"] - df[\"card_amt_mean\"]) /\n",
    "    (df[\"card_amt_std\"] + 1e-6)\n",
    ")\n",
    "\n",
    "\n",
    "# Distance géographique entre transactions consécutives\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # rayon Terre en km\n",
    "\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat / 2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "\n",
    "# Tri chronologique des transactions par carte\n",
    "df = df.sort_values([\"card_id\", \"unix_trans_time\"])\n",
    "# Coordonnées du marchand de la transaction précédente\n",
    "df[\"prev_merch_lat\"] = df.groupby(\"card_id\")[\"merch_lat\"].shift()\n",
    "df[\"prev_merch_long\"] = df.groupby(\"card_id\")[\"merch_long\"].shift()\n",
    "# Distance géographique entre deux transactions consécutives\n",
    "# Une grande distance sur un temps court est un fort signal de fraude\n",
    "df[\"geo_dist\"] = haversine_np(\n",
    "    df[\"merch_lat\"], df[\"merch_long\"],\n",
    "    df[\"prev_merch_lat\"], df[\"prev_merch_long\"]\n",
    ")\n",
    "# Valeur nulle pour la première transaction\n",
    "df[\"geo_dist\"] = df[\"geo_dist\"].fillna(0)\n",
    "\n",
    "# Nouveau merchant pour la carte\n",
    "# Indique si ce marchand n’a jamais été utilisé auparavant\n",
    "# par cette carte (1 = nouveau marchand)\n",
    "df[\"is_new_merchant\"] = (\n",
    "    df.groupby(\"card_id\")[\"merchant\"]\n",
    "    .transform(lambda x: ~x.duplicated())\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Dynamique temporelle du merchant\n",
    "# Temps écoulé depuis la dernière transaction chez ce marchand\n",
    "# Un afflux soudain de transactions peut être suspect\n",
    "df = df.sort_values([\"merchant_id\", \"unix_trans_time\"])\n",
    "df[\"merchant_time_since_prev_tx\"] = (\n",
    "    df.groupby(\"merchant_id\")[\"unix_trans_time\"]\n",
    "    .diff()\n",
    "    .fillna(FEATURE_WINDOW)\n",
    ")\n",
    "\n",
    "# Montant moyen historique des transactions du merchant\n",
    "# (calculé uniquement sur le passé)\n",
    "df[\"merchant_avg_amt\"] = (\n",
    "    df.groupby(\"merchant_id\")[\"amt\"]\n",
    "    .expanding()\n",
    "    .mean()\n",
    "    .shift()\n",
    "    .reset_index(level=0, drop=True)\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7434",
   "metadata": {},
   "source": [
    "# Create PyG graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac6c49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, MAX_EDGES = 5, EDGE_WINDOW = 3600 * 24 * 7): #7 jours\n",
    "    \"\"\"\n",
    "    Crée un graphe PyG à partir d'un dataframe df.\n",
    "    \"\"\"\n",
    "    # Node features\n",
    "    node_features = torch.tensor(\n",
    "        df[[\n",
    "            # Transaction features\n",
    "            \"amt\",\n",
    "            \"card_time_since_prev_tx\",\n",
    "            \"hour\",\n",
    "            \"dayofweek\",\n",
    "            \"is_weekend\",\n",
    "            \"age\",\n",
    "            \"is_new_merchant\",\n",
    "            \"geo_dist\",\n",
    "            \"merch_lat\", \"merch_long\",\n",
    "            \"category_idx\",\n",
    "\n",
    "            # Card features\n",
    "            \"amt_zscore\",\n",
    "            \"card_amt_mean\",\n",
    "            \"card_amt_std\",\n",
    "            \"gender_idx\", \"job_idx\",\n",
    "            \"lat\", \"long\", \"city_pop\",\n",
    "\n",
    "            # Mercahnt feature\n",
    "            \"merchant_avg_amt\",\n",
    "            \"merchant_time_since_prev_tx\"\n",
    "            \n",
    "        ]].values,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    node_labels = torch.tensor(\n",
    "        df[\"is_fraud\"].values,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "\n",
    "    # Mapping transaction_id -> index PyG\n",
    "    tx2idx = {tx: i for i, tx in enumerate(df[\"transaction_id\"].values)}\n",
    "\n",
    "    # Create edges\n",
    "    edges = []\n",
    "    edge_types = []\n",
    "\n",
    "    # Création des arêtes pour transactions de la même carte\n",
    "    # On regroupe les transactions par carte (card_id)\n",
    "    # puis on relie les transactions consécutives dans la fenêtre EDGE_WINDOW\n",
    "    for _, group in df.groupby(\"card_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t = group[\"unix_trans_time\"].values\n",
    "        \n",
    "        n = len(tx)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, min(i + 1 + MAX_EDGES, n)):\n",
    "                dt = t[j] - t[i]\n",
    "                if dt <= EDGE_WINDOW:\n",
    "                    # On ajoute une arête bidirectionnelle\n",
    "                    edges.append([tx2idx[tx[i]], tx2idx[tx[j]]])\n",
    "                    # Attribut de l'arête : [same_card, same_merchant]\n",
    "                    # Ici, same_card = 1, same_merchant = \n",
    "                    edge_types.append(0)\n",
    "\n",
    "\n",
    "    # Création des arêtes pour transactions du même merchant\n",
    "    # Même logique que pour les cartes\n",
    "    # On relie les transactions consécutives chez le même marchand\n",
    "    for _, group in df.groupby(\"merchant_id\"):\n",
    "        # Tri chronologique des transactions\n",
    "        group = group.sort_values(\"unix_trans_time\")\n",
    "        tx = group[\"transaction_id\"].values\n",
    "        t = group[\"unix_trans_time\"].values\n",
    "\n",
    "        n = len(tx)\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, min(i + 1 + MAX_EDGES, n)):\n",
    "                dt = t[j] - t[i]\n",
    "\n",
    "                if dt <= EDGE_WINDOW:\n",
    "                    edges.append([tx2idx[tx[i]], tx2idx[tx[j]]])  # i -> j\n",
    "                    # Attribut de l'arête : [same_card, same_merchant]\n",
    "                    # Ici, same_card = 0, same_merchant = 1\n",
    "                    edge_types.append(1)\n",
    "\n",
    "    \n",
    "    # Assemble Data\n",
    "    # Conversion des arêtes et attributs en tenseurs PyTorch\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
    "\n",
    "    # Création du graphe PyTorch Geometric\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_type=edge_type,\n",
    "        y=node_labels\n",
    "    )\n",
    "\n",
    "    data.num_nodes = node_features.size(0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c75c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 1 ---\n",
      "Data(x=[103878, 21], edge_index=[2, 982538], y=[103878], edge_type=[982538], num_nodes=103878)\n",
      "Nombre de noeuds: 103878\n",
      "Nombre d'arêtes: 982538\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 2 ---\n",
      "Data(x=[96804, 21], edge_index=[2, 912576], y=[96804], edge_type=[912576], num_nodes=96804)\n",
      "Nombre de noeuds: 96804\n",
      "Nombre d'arêtes: 912576\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 3 ---\n",
      "Data(x=[142851, 21], edge_index=[2, 1375935], y=[142851], edge_type=[1375935], num_nodes=142851)\n",
      "Nombre de noeuds: 142851\n",
      "Nombre d'arêtes: 1375935\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 4 ---\n",
      "Data(x=[134292, 21], edge_index=[2, 1290725], y=[134292], edge_type=[1290725], num_nodes=134292)\n",
      "Nombre de noeuds: 134292\n",
      "Nombre d'arêtes: 1290725\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 5 ---\n",
      "Data(x=[145940, 21], edge_index=[2, 1407458], y=[145940], edge_type=[1407458], num_nodes=145940)\n",
      "Nombre de noeuds: 145940\n",
      "Nombre d'arêtes: 1407458\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 6 ---\n",
      "Data(x=[143123, 21], edge_index=[2, 1381680], y=[143123], edge_type=[1381680], num_nodes=143123)\n",
      "Nombre de noeuds: 143123\n",
      "Nombre d'arêtes: 1381680\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 7 ---\n",
      "Data(x=[86265, 21], edge_index=[2, 837406], y=[86265], edge_type=[837406], num_nodes=86265)\n",
      "Nombre de noeuds: 86265\n",
      "Nombre d'arêtes: 837406\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 8 ---\n",
      "Data(x=[86977, 21], edge_index=[2, 844820], y=[86977], edge_type=[844820], num_nodes=86977)\n",
      "Nombre de noeuds: 86977\n",
      "Nombre d'arêtes: 844820\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 9 ---\n",
      "Data(x=[70234, 21], edge_index=[2, 676066], y=[70234], edge_type=[676066], num_nodes=70234)\n",
      "Nombre de noeuds: 70234\n",
      "Nombre d'arêtes: 676066\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 10 ---\n",
      "Data(x=[68304, 21], edge_index=[2, 656846], y=[68304], edge_type=[656846], num_nodes=68304)\n",
      "Nombre de noeuds: 68304\n",
      "Nombre d'arêtes: 656846\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 11 ---\n",
      "Data(x=[70033, 21], edge_index=[2, 674286], y=[70033], edge_type=[674286], num_nodes=70033)\n",
      "Nombre de noeuds: 70033\n",
      "Nombre d'arêtes: 674286\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "--- Batch 12 ---\n",
      "Data(x=[140468, 21], edge_index=[2, 1380625], y=[140468], edge_type=[1380625], num_nodes=140468)\n",
      "Nombre de noeuds: 140468\n",
      "Nombre d'arêtes: 1380625\n",
      "Dimension features noeuds: 21\n",
      "Nombre de type d'arêtes: 2\n",
      "\n",
      "12 graphes créés, un pour chaque période de 1 mois.\n"
     ]
    }
   ],
   "source": [
    "# durée de chaque sous-graphe (batch) en mois\n",
    "nb_months = 1\n",
    "\n",
    "# Découpage par nb_months\n",
    "graphs = []\n",
    "df[\"batch_index\"] = df[\"unix_trans_time\"].apply(lambda x: (pd.to_datetime(x, unit='s').month - 1)//nb_months + 1)\n",
    "for period, period_df in df.groupby(\"batch_index\"):\n",
    "    graph = create_graph(period_df.reset_index(drop=True))\n",
    "    graphs.append(graph)\n",
    "    print(f\"--- Batch {period} ---\")\n",
    "    print(graph)\n",
    "    print(\"Nombre de noeuds:\", graph.num_nodes)\n",
    "    print(\"Nombre d'arêtes:\", graph.num_edges)\n",
    "    print(\"Dimension features noeuds:\", graph.x.shape[1])\n",
    "    print(\"Nombre de type d'arêtes:\", len(torch.unique(graph.edge_type)))\n",
    "    print()\n",
    "\n",
    "print(f\"{len(graphs)} graphes créés, un pour chaque période de {nb_months} mois.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e6d025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe batch 1 enregistré : graphs/train/graph_batch_1.pt\n",
      "Graphe batch 2 enregistré : graphs/train/graph_batch_2.pt\n",
      "Graphe batch 3 enregistré : graphs/train/graph_batch_3.pt\n",
      "Graphe batch 4 enregistré : graphs/train/graph_batch_4.pt\n",
      "Graphe batch 5 enregistré : graphs/train/graph_batch_5.pt\n",
      "Graphe batch 6 enregistré : graphs/train/graph_batch_6.pt\n",
      "Graphe batch 7 enregistré : graphs/train/graph_batch_7.pt\n",
      "Graphe batch 8 enregistré : graphs/train/graph_batch_8.pt\n",
      "Graphe batch 9 enregistré : graphs/train/graph_batch_9.pt\n",
      "Graphe batch 10 enregistré : graphs/train/graph_batch_10.pt\n",
      "Graphe batch 11 enregistré : graphs/train/graph_batch_11.pt\n",
      "Graphe batch 12 enregistré : graphs/train/graph_batch_12.pt\n"
     ]
    }
   ],
   "source": [
    "# Save to a file\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, graph in enumerate(graphs, start=1):\n",
    "    file_path = os.path.join(save_dir, f\"graph_batch_{i}.pt\")\n",
    "    torch.save(graph, file_path)\n",
    "    print(f\"Graphe batch {i} enregistré : {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGAE",
   "language": "python",
   "name": "vgae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
